{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a2ce82-05ec-4163-97b1-57f7ef3e2f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session (only if needed outside Databricks notebooks)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define output path\n",
    "output_path = \"/mnt/input-data/streaming-data/\"\n",
    "\n",
    "# Sample data pools\n",
    "emp_names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Helen\", \"Ivy\", \"Jack\", \"Karen\", \"Leo\", \"Mona\", \"Nina\", \"Oscar\", \"Paul\", \"Quincy\", \"Rita\", \"Steve\", \"Tina\"]\n",
    "countries = [\"USA\", \"Canada\", \"UK\", \"India\", \"Germany\"]\n",
    "designations = [\"Software Engineer\", \"Data Scientist\", \"Manager\", \"Analyst\", \"Consultant\"]\n",
    "departments = [\"IT\", \"HR\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "\n",
    "# Counter to create unique employee IDs\n",
    "empid_counter = 1000\n",
    "\n",
    "# Function to generate a batch of employee records\n",
    "def generate_batch_records(batch_size=10):\n",
    "    global empid_counter\n",
    "    records = []\n",
    "    selected_names = random.sample(emp_names, batch_size)  # Select unique names for this batch\n",
    "\n",
    "    for name in selected_names:\n",
    "        record = {\n",
    "            \"empid\": empid_counter,\n",
    "            \"empname\": name,\n",
    "            \"country\": random.choice(countries),\n",
    "            \"designation\": random.choice(designations),\n",
    "            \"dept\": random.choice(departments)\n",
    "        }\n",
    "        records.append(record)\n",
    "        empid_counter += 1\n",
    "\n",
    "    return records\n",
    "\n",
    "# Write multiple files, each with 10 records\n",
    "for i in range(100):  # You can change this to create more files\n",
    "    batch_records = generate_batch_records(batch_size=10)\n",
    "    json_data = \"\\n\".join([json.dumps(record) for record in batch_records])  # Write as newline-delimited JSON (standard practice)\n",
    "    \n",
    "    file_path = f\"{output_path}employee_batch_{i}.json\"\n",
    "    \n",
    "    # Save JSON batch into a file\n",
    "    dbutils.fs.put(file_path, json_data, overwrite=True)\n",
    "    \n",
    "    print(f\"Written file: {file_path} with {len(batch_records)} records.\")\n",
    "    \n",
    "    time.sleep(3)  # wait 3 seconds between file writes to simulate streaming\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "JSON GEN",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
