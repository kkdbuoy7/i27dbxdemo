{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa17d0b-053d-4b12-a23e-862cac793539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec625959-24a5-4b5d-b7a0-2ba97a185223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Read source data from CSV\n",
    "# source_df = spark.read.option(\"header\", True).option(\"inferschema\", True).csv(\"/mnt/input-data/employees.csv\");\n",
    "\n",
    "\n",
    "# source_df = source_df.filter(col(\"EMPLOYEE_ID\")> 200)\n",
    "\n",
    "# source_df.display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c14d35-a869-47cf-9361-134e70878363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# source_df.write.format(\"delta\")\\\n",
    "#     .mode(\"overwrite\")\\\n",
    "#         .save(\"/tmp/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0995f131-34df-4511-a1d3-3fdf16b84135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# create table if not exists employees\n",
    "# using delta;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd567a5-0467-4c00-94b0-d9d081dea5cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df22 = spark.read.format(\"delta\").load(\"/tmp/employees\")\n",
    "df22.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c580f6-05f6-48ac-ac8a-3de2c835a0ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read source data from CSV\n",
    "source_df = spark.read.option(\"header\", True).option(\"inferschema\", True).csv(\"/mnt/input-data/employees.csv\")\n",
    "\n",
    "# source_df.display()\n",
    "# Read the existing target Delta table as a DataFrame\n",
    "target_df = spark.read.format(\"delta\").load(\"/tmp/employees\")\n",
    "\n",
    "source_df.display()\n",
    "target_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304f20cc-d9a8-49e6-a89c-9b12f922b8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join source and target on employee_id\n",
    "joined_df = source_df.alias(\"src\").join(\n",
    "    target_df.alias(\"tgt\"),\n",
    "    on=\"EMPLOYEE_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Find records that need to be updated (any column changed)\n",
    "updates_df = joined_df.filter(\n",
    "    (joined_df[\"src.FIRST_NAME\"] != joined_df[\"tgt.FIRST_NAME\"]) |\n",
    "    (joined_df[\"src.LAST_NAME\"] != joined_df[\"tgt.LAST_NAME\"]) |\n",
    "    (joined_df[\"src.DEPARTMENT_ID\"] != joined_df[\"tgt.DEPARTMENT_ID\"]) |\n",
    "    (joined_df[\"src.SALARY\"] != joined_df[\"tgt.SALARY\"])\n",
    ").select(\"src.*\")\n",
    "\n",
    "# Find records that are new (not present in target)\n",
    "new_records_df = source_df.join(target_df, on=\"EMPLOYEE_ID\", how=\"left_anti\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05373a0-7468-407d-95ef-2116b182b8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove old versions of updated records from the target\n",
    "unchanged_df = target_df.join(updates_df, on=\"EMPLOYEE_ID\", how=\"left_anti\")\n",
    "\n",
    "# Combine unchanged + updated + new records\n",
    "final_df = unchanged_df.union(updates_df).union(new_records_df)\n",
    "\n",
    "# Overwrite the target Delta table\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a690daf3-be0e-4dd5-8715-584a53af51e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df22 = spark.read.format(\"delta\").load(\"/tmp/employees\")\n",
    "df22.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e80f7b67-e80c-495c-b73c-9ca0ac4ec5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6384980130074807,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD TYPE 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
